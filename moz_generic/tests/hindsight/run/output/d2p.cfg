filename        = "s3_parquet.lua"
message_matcher = "Type == 'moz_generic' && Fields[namespace] == 'generic' && Fields[docType] == 'test' && Fields[docVersion] == '1'"
ticker_interval = 60
preserve_data   = true

parquet_schema = [=[
message test {
  required binary foo (UTF8);
  required group metadata {
      required int64  Timestamp;
      required binary documentId (UTF8);
      optional binary geoCountry (UTF8);
      optional binary geoCity (UTF8);
  }
}]=]

metadata_group = "metadata"
json_objects = {"Fields[submission]"}
s3_path_dimensions  = {
    {name = "_submission_date", source = "Timestamp", dateformat = "%Y%m%d", scaling_factor = 1e-9}
}

-- directory location to store the intermediate output files
batch_dir           = "output/parquet"
max_writers         = 5
max_rowgroup_size   = 10000.
max_file_size       = 1024 * 1024 * 300
max_file_age        = 60 * 60
hive_compatible     = true -- default false
